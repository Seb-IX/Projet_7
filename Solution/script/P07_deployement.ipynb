{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89466231",
   "metadata": {},
   "source": [
    "# Déployement sur Azure Machine Learning du modèle final\n",
    "\n",
    "<img src=\"./img/Microsoft-Azure-Logo.png\" style=\"max-height: 200px; margin-left:auto; margin-right:auto; display: block;\">\n",
    "\n",
    "Ce notebook permet le déploiement du modèle avancé sur Azure Machine Learning.\n",
    "\n",
    "Pour ça il est nécessaire de créer une ressource Azure Machine Learning sur le <a href=\"https://portal.azure.com/#create/Microsoft.MachineLearningServices\">\"portal.azure.com\"</a> et de créer les variables d'environnement de l'abonnement ID (SUBSCRIPTION_ML), le nom du gorupe de ressource (RESOURCE_GROUP_ML), et le nom de l'espace de travail Azure Machine Learning fraichement créé (WORKSPACE_ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# import mlflow\n",
    "# import mlflow.azureml\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a600d40",
   "metadata": {},
   "source": [
    "## Connexion à l'espace de travail Azure Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94ce34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = os.environ[\"SUBSCRIPTION_ML\"]\n",
    "resource_group = os.environ[\"RESOURCE_GROUP_ML\"]\n",
    "workspace_name = os.environ[\"WORKSPACE_ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abfd506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id=subscription_id,\n",
    "               resource_group=resource_group,\n",
    "               workspace_name=workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbea1a",
   "metadata": {},
   "source": [
    "## Test du modèle sauvegardé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f3b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "path_to_model=\"./data/final_model\"\n",
    "\n",
    "tokenise = joblib.load(path_to_model+\"/tokenizer.pkl\")\n",
    "model = load_model(path_to_model+\"/w2v_model\")\n",
    "\n",
    "def preprocess_data(data : list):\n",
    "    return pad_sequences(tokenise.texts_to_sequences([cleanning_data(tweet) for tweet in data]),maxlen=300)\n",
    "\n",
    "def cleanning_data(sentence):\n",
    "    sentence += \" \"\n",
    "    sentence = re.sub('\\n+', ' ', sentence.lower())\n",
    "    # special caractère\n",
    "    sentence = re.sub('[éèêë]','e',sentence)\n",
    "    sentence = re.sub('[ïîì]','i',sentence)\n",
    "    sentence = re.sub('[àâä]','a',sentence)\n",
    "    sentence = re.sub('[ôö]','o',sentence) \n",
    "    sentence = re.sub('[üûù]','u',sentence)\n",
    "    sentence = re.sub('[ç]','c',sentence)\n",
    "    # personal_clean\n",
    "    sentence = clean_escape(sentence) # usless ?\n",
    "    sentence = clean_url(sentence)\n",
    "    sentence = clean_smile(sentence)\n",
    "    sentence = re.sub(r'[^a-z@#\\'_ ]+',' ',sentence)\n",
    "    sentence = clean_uniuque_char(sentence)\n",
    "    sentence = re.sub(' {2,}', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_uniuque_char(sentence):\n",
    "    sentence = \" \".join([v for v in sentence.split(\" \") if ((len(v) > 1) or (not v.isalpha()))])\n",
    "    return sentence\n",
    "\n",
    "def clean_escape(sentence):\n",
    "    sentence = re.sub('&amp;','&',sentence)\n",
    "    sentence = re.sub('&quot;','\\\"',sentence)\n",
    "    sentence = re.sub('&gt;','>',sentence)\n",
    "    sentence = re.sub('&lt;','<',sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_smile(sentence):\n",
    "    sentence = re.sub(';\\)','emote_clin_doeil ',sentence)\n",
    "    sentence = re.sub(':\\)|=\\)','emote_sourire ',sentence)\n",
    "    sentence = re.sub('x\\)','emote_cross_smile ',sentence)\n",
    "    sentence = re.sub('x{1,}[Dd]{1,}[^A-Za-z0-9]','emote_mdr ',sentence)\n",
    "    sentence = re.sub(';[pP]{1,}[^A-Za-z0-9]|:[pP]{1,}[^A-Za-z0-9]|x[pP]{1,}[^A-Za-z0-9]|=[pP]{1,}[^A-Za-z0-9]',\n",
    "                      'emote_tire_langue ',sentence)\n",
    "    sentence = re.sub('[oO]_[oO]','emote_tres_surpris',sentence)\n",
    "    sentence = re.sub('[Xx]{1,}[Oo]{1,}[^A-Za-z0-9]','emote_ko ',sentence)\n",
    "    sentence = re.sub('[bB]\\)[^A-Za-z0-9]','emote_lunette ',sentence)\n",
    "    sentence = re.sub('[=:;][oO]{1,}[^A-Za-z0-9]','emote_surpris ',sentence)\n",
    "    sentence = re.sub('[=:;][S]{1,}[^A-Za-z0-9]','emote_genant ',sentence)\n",
    "    sentence = re.sub('[:;=][dD]{1,}[^A-Za-z0-9]','emote_gros_sourire ',sentence)\n",
    "    sentence = re.sub('[:;=][$]{1,}[^A-Za-z0-9]','emote_genant_discret ',sentence)\n",
    "    sentence = re.sub('[:;=]\\|[^A-Za-z0-9]','emote_neutre ',sentence)\n",
    "    sentence = re.sub('[:;=]/[^A-Za-z0-9]','emote_deception ',sentence)\n",
    "    sentence = re.sub('[:;=]\\([^A-Za-z0-9]','emote_insatisfait ',sentence)\n",
    "    sentence = re.sub('\\*[-_]\\*[^A-Za-z0-9]','emote_magnifique ',sentence)\n",
    "    sentence = re.sub('\\^[-_]{0,1}\\^[^A-Za-z0-9]','emote_joyeux ',sentence)\n",
    "    sentence = re.sub('--\\'[^A-Za-z0-9]','emote_enerve ',sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_url(sentence):\n",
    "    sentence = re.sub('http[s]*://[0-9a-zA-Z-_.]*.[a-z]{0,3}/[0-9a-zA-Z-_./]*', '', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de64692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data([\"realy ?\",\"I love banana!\",\"I hate banana!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "530f9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(data)\n",
    "y_pred = np.argmax(model.predict(data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9d8f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n",
      "---------\n",
      "[[0.5611326  0.43886733]\n",
      " [0.05260534 0.9473947 ]\n",
      " [0.95672864 0.04327139]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(\"---------\")\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f6016",
   "metadata": {},
   "source": [
    "## Upload du modèle Keras et du Tokenizer sur l'espace de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34702667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model tokenizer_keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_azure = Model.register(model_path=path_to_model+\"/tokenizer.pkl\",\n",
    "                       model_name=\"tokenizer_keras\",\n",
    "                       tags={'area': \"NLP\", 'type': \"tokenizer\"},\n",
    "                       description=\"Tokenizer entrainer avec le jeu de données final.\",\n",
    "                       workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b695cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model keras_w2v_model\n"
     ]
    }
   ],
   "source": [
    "w2v_model_azure = Model.register(model_path=path_to_model+\"/w2v_model\",\n",
    "                       model_name=\"keras_w2v_model\",\n",
    "                       tags={'area': \"NLP\", 'type': \"deep_learning\"},\n",
    "                       description=\"Word2Vec LSTM modèle keras entrainé\",\n",
    "                       workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de récupérer l'instance cloud du modèle si il existe déjà dans l'espace de travail\n",
    "# Il est également possible d'indiquer la version avec l'argument \"version=2\"\n",
    "\n",
    "# tokenizer_azure = Model(ws, 'tokenizer_keras')\n",
    "# w2v_model_azure = Model(ws, 'keras_w2v_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01b84b",
   "metadata": {},
   "source": [
    "## Création du script d'entrée de l'inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2882c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_directory = \"./advanced_model\"\n",
    "\n",
    "os.makedirs(source_directory, exist_ok=True)\n",
    "os.makedirs(os.path.join(source_directory, \"x/y\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a06179d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting advanced_model/x/y/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile advanced_model/x/y/score.py\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    global tokenise\n",
    "\n",
    "    tokenizer_path = Model.get_model_path(model_name = 'tokenizer_keras')\n",
    "    keras_path = Model.get_model_path(model_name = 'keras_w2v_model')\n",
    "\n",
    "    try:\n",
    "        tokenise = joblib.load(tokenizer_path)\n",
    "    except:\n",
    "        print(\"tokenizer not work\")\n",
    "        \n",
    "    try: \n",
    "        model = load_model(keras_path)\n",
    "    except:\n",
    "        print(\"Keras model not working\")\n",
    "\n",
    "def run(request):\n",
    "    try:\n",
    "        req = json.loads(request)\n",
    "        data = req[\"data\"]\n",
    "        method = req[\"method\"] \n",
    "        if type(data) is list:\n",
    "            data = preprocess_data(data)\n",
    "        else:\n",
    "            return \"Please enter list of tweet as type of \\\"list\\\"\"\n",
    "            \n",
    "        if str(method) == \"prediction\":\n",
    "            dic = {1:\"good\",0:\"bad\"}\n",
    "            result = np.argmax(model.predict(data),axis=1)\n",
    "            return [dic[y] for y in result]\n",
    "            \n",
    "        elif str(method) == \"score\":\n",
    "            result = model.predict(data)\n",
    "            return result[:,1].tolist()\n",
    "        \n",
    "        elif str(method) == \"prediction_with_neutral\":\n",
    "            result = model.predict(data)\n",
    "            result = result[:,1]\n",
    "            return [predict_with_neutral(y) for y in result]\n",
    "        else:\n",
    "            return \"\"\"method aviable : \\\"prediction\\\",\\\"score\\\",\\\"prediction_with_neutral\\\"\n",
    " - prediction return labels sentiment (bad/good)\n",
    " - score return score of sentiment 0(bad) to 1(good)\n",
    " - prediction_with_neutral return labels sentiment with label neutral\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n",
    "    \n",
    "def predict_with_neutral(score):\n",
    "    if score >= 0.6:\n",
    "        return \"good\"\n",
    "    elif score <= 0.4:\n",
    "        return \"bad\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def preprocess_data(data : list):\n",
    "    return pad_sequences(tokenise.texts_to_sequences([cleanning_data(tweet) for tweet in data]),maxlen=300)\n",
    "\n",
    "def cleanning_data(sentence):\n",
    "    sentence += \" \"\n",
    "    sentence = re.sub('\\n+', ' ', sentence.lower())\n",
    "    # special caractère\n",
    "    sentence = re.sub('[éèêë]','e',sentence)\n",
    "    sentence = re.sub('[ïîì]','i',sentence)\n",
    "    sentence = re.sub('[àâä]','a',sentence)\n",
    "    sentence = re.sub('[ôö]','o',sentence) \n",
    "    sentence = re.sub('[üûù]','u',sentence)\n",
    "    sentence = re.sub('[ç]','c',sentence)\n",
    "    # personal_clean\n",
    "    sentence = clean_escape(sentence) # usless ?\n",
    "    sentence = clean_url(sentence)\n",
    "    sentence = clean_smile(sentence)\n",
    "    sentence = re.sub(r'[^a-z@#\\'_ ]+',' ',sentence)\n",
    "    sentence = clean_uniuque_char(sentence)\n",
    "    sentence = re.sub(' {2,}', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_uniuque_char(sentence):\n",
    "    sentence = \" \".join([v for v in sentence.split(\" \") if ((len(v) > 1) or (not v.isalpha()))])\n",
    "    return sentence\n",
    "\n",
    "def clean_escape(sentence):\n",
    "    sentence = re.sub('&amp;','&',sentence)\n",
    "    sentence = re.sub('&quot;','\\\"',sentence)\n",
    "    sentence = re.sub('&gt;','>',sentence)\n",
    "    sentence = re.sub('&lt;','<',sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_smile(sentence):\n",
    "    sentence = re.sub(';\\)','emote_clin_doeil ',sentence)\n",
    "    sentence = re.sub(':\\)|=\\)','emote_sourire ',sentence)\n",
    "    sentence = re.sub('x\\)','emote_cross_smile ',sentence)\n",
    "    sentence = re.sub('x{1,}[Dd]{1,}[^A-Za-z0-9]','emote_mdr ',sentence)\n",
    "    sentence = re.sub(';[pP]{1,}[^A-Za-z0-9]|:[pP]{1,}[^A-Za-z0-9]|x[pP]{1,}[^A-Za-z0-9]|=[pP]{1,}[^A-Za-z0-9]',\n",
    "                      'emote_tire_langue ',sentence)\n",
    "    sentence = re.sub('[oO]_[oO]','emote_tres_surpris',sentence)\n",
    "    sentence = re.sub('[Xx]{1,}[Oo]{1,}[^A-Za-z0-9]','emote_ko ',sentence)\n",
    "    sentence = re.sub('[bB]\\)[^A-Za-z0-9]','emote_lunette ',sentence)\n",
    "    sentence = re.sub('[=:;][oO]{1,}[^A-Za-z0-9]','emote_surpris ',sentence)\n",
    "    sentence = re.sub('[=:;][S]{1,}[^A-Za-z0-9]','emote_genant ',sentence)\n",
    "    sentence = re.sub('[:;=][dD]{1,}[^A-Za-z0-9]','emote_gros_sourire ',sentence)\n",
    "    sentence = re.sub('[:;=][$]{1,}[^A-Za-z0-9]','emote_genant_discret ',sentence)\n",
    "    sentence = re.sub('[:;=]\\|[^A-Za-z0-9]','emote_neutre ',sentence)\n",
    "    sentence = re.sub('[:;=]/[^A-Za-z0-9]','emote_deception ',sentence)\n",
    "    sentence = re.sub('[:;=]\\([^A-Za-z0-9]','emote_insatisfait ',sentence)\n",
    "    sentence = re.sub('\\*[-_]\\*[^A-Za-z0-9]','emote_magnifique ',sentence)\n",
    "    sentence = re.sub('\\^[-_]{0,1}\\^[^A-Za-z0-9]','emote_joyeux ',sentence)\n",
    "    sentence = re.sub('--\\'[^A-Za-z0-9]','emote_enerve ',sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_url(sentence):\n",
    "    sentence = re.sub('http[s]*://[0-9a-zA-Z-_.]*.[a-z]{0,3}/[0-9a-zA-Z-_./]*', '', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1ed25",
   "metadata": {},
   "source": [
    "## Création de l'inférence & environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4849d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "env_tf_sk = Environment('tensorflow-scikit')\n",
    "env_tf_sk.python.conda_dependencies.add_pip_package(\"joblib\")\n",
    "env_tf_sk.python.conda_dependencies.add_pip_package(\"scikit-learn\")\n",
    "env_tf_sk.python.conda_dependencies.add_pip_package(\"tensorflow\")\n",
    "env_tf_sk.python.conda_dependencies.add_pip_package(\"azureml-core\")\n",
    "\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=source_directory,\n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   environment=env_tf_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99de3f7",
   "metadata": {},
   "source": [
    "## Configuration ACI et déploiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3ac093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-11-24 21:31:19+01:00 Creating Container Registry if not exists.\n",
      "2021-11-24 21:31:19+01:00 Registering the environment.\n",
      "2021-11-24 21:31:21+01:00 Use the existing image.\n",
      "2021-11-24 21:31:21+01:00 Generating deployment configuration.\n",
      "2021-11-24 21:31:23+01:00 Submitting deployment to compute.\n",
      "2021-11-24 21:31:28+01:00 Checking the status of deployment tweet-sentiment-analysis..\n",
      "2021-11-24 21:33:29+01:00 Checking the status of inference endpoint tweet-sentiment-analysis.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "# Set deployment configuration\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 2, memory_gb = 4,auth_enabled=True)\n",
    "\n",
    "# Define the model, inference, & deployment configuration and web service name and location to deploy\n",
    "service = Model.deploy(\n",
    "    workspace = ws,\n",
    "    name = \"tweet-sentiment-analysis\",\n",
    "    models = [tokenizer_azure,w2v_model_azure],\n",
    "    inference_config = inference_config,\n",
    "    deployment_config = deployment_config)\n",
    "\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deaafdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aocNmbc0clYZ1ToPtcTeJl9UPZlkJMDV\n",
      "http://212175d7-6cbf-47be-a1d7-6f3ae6316d35.francecentral.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "primary, secondary = service.get_keys()\n",
    "print(primary)\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f5545",
   "metadata": {},
   "source": [
    "## Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "413068a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'good', 'bad']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "input_payload = json.dumps({\n",
    "    'data': [\"realy ?\",\"I love banana!\",\"I hate banana!\"],\n",
    "    'method':\"prediction_with_neutral\"\n",
    "})\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad371ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_payload = json.dumps({\n",
    "    'data': [\"realy ?\",\"I love banana!\",\"I hate banana!\"],\n",
    "    'method':\"score\"\n",
    "})\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_payload = json.dumps({\n",
    "    'data': [\"realy ?\",\"I love banana!\",\"I hate banana!\"],\n",
    "    'method':\"prediction\"\n",
    "})\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483785f",
   "metadata": {},
   "source": [
    "## Suppression du service\n",
    "\n",
    "Permet d'éviter les frais quand le service n'est plus \"utile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce778000",
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
